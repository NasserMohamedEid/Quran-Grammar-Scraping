{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Verse              Analysis\n",
      "0                              (١) سورة الفاتحة  No analysis provided\n",
      "1      ﴿بِسْمِ اللهِ الرَّحْمنِ الرَّحِيمِ (١)﴾  No analysis provided\n",
      "2     ﴿الْحَمْدُ لِلّهِ رَبِّ الْعالَمِينَ (٢)﴾  No analysis provided\n",
      "3                   ﴿الرَّحْمنِ الرَّحِيمِ (٣)﴾  No analysis provided\n",
      "4                  ﴿مالِكِ يَوْمِ الدِّينِ (٤)﴾  No analysis provided\n",
      "5  ﴿إِيّاكَ نَعْبُدُ وَإِيّاكَ نَسْتَعِينُ (٥)﴾  No analysis provided\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd  # Import pandas for DataFrame\n",
    "\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = \"https://app.turath.io/book/86\"\n",
    "driver.get(url)\n",
    "\n",
    "def extract_page_content(page_id):\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, page_id)))  # Adjust as needed\n",
    "\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "        page_div = soup.find('div', {'id': page_id})\n",
    "\n",
    "        verses = []\n",
    "        analysis = []\n",
    "\n",
    "        # Find all sections for verses\n",
    "        for verse_section in page_div.find_all('span', {'data-type': 'title'}):\n",
    "            # Extract verse text\n",
    "            verse_text = verse_section.get_text(strip=True)\n",
    "            \n",
    "            # Extract analysis\n",
    "            analysis_section = verse_section.find_next_sibling('span')\n",
    "            while analysis_section and analysis_section.name == 'span' and 'الإعراب' not in analysis_section.get_text():\n",
    "                analysis_section = analysis_section.find_next_sibling('span')\n",
    "            \n",
    "            if analysis_section:\n",
    "                analysis_text = analysis_section.get_text(strip=True).replace('\\n', ' ').replace('  ', ' ')\n",
    "            else:\n",
    "                analysis_text = 'No analysis provided'\n",
    "            \n",
    "            verses.append(verse_text)\n",
    "            analysis.append(analysis_text)\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Verse': verses,\n",
    "            'Analysis': analysis\n",
    "        })\n",
    "\n",
    "        print(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function with the appropriate page ID\n",
    "extract_page_content(\"pg-19\")  # Replace with the actual page ID\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=128.0.6613.138)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7CF2C9412+29090]\n",
      "\t(No symbol) [0x00007FF7CF23E239]\n",
      "\t(No symbol) [0x00007FF7CF0FB1DA]\n",
      "\t(No symbol) [0x00007FF7CF0CFAF5]\n",
      "\t(No symbol) [0x00007FF7CF17E2C7]\n",
      "\t(No symbol) [0x00007FF7CF195EB1]\n",
      "\t(No symbol) [0x00007FF7CF176493]\n",
      "\t(No symbol) [0x00007FF7CF1409D1]\n",
      "\t(No symbol) [0x00007FF7CF141B31]\n",
      "\tGetHandleVerifier [0x00007FF7CF5E871D+3302573]\n",
      "\tGetHandleVerifier [0x00007FF7CF634243+3612627]\n",
      "\tGetHandleVerifier [0x00007FF7CF62A417+3572135]\n",
      "\tGetHandleVerifier [0x00007FF7CF385EB6+801862]\n",
      "\t(No symbol) [0x00007FF7CF24945F]\n",
      "\t(No symbol) [0x00007FF7CF244FB4]\n",
      "\t(No symbol) [0x00007FF7CF245140]\n",
      "\t(No symbol) [0x00007FF7CF23461F]\n",
      "\tBaseThreadInitThunk [0x00007FFCB0B17374+20]\n",
      "\tRtlUserThreadStart [0x00007FFCB1DDCC91+33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd  # Import pandas for DataFrame\n",
    "\n",
    "# sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = \"https://app.turath.io/book/86\"\n",
    "driver.get(url)\n",
    "\n",
    "def extract_page_content(page_id):\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, page_id)))  # Adjust as needed\n",
    "\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "        page_div = soup.find('div', {'id': page_id})\n",
    "\n",
    "        verses = []\n",
    "        analysis = []\n",
    "\n",
    "        # Find all sections for verses\n",
    "        for verse_section in page_div.find_all('span', {'data-type': 'title'}):\n",
    "            # Extract verse text\n",
    "            verse_text = verse_section.get_text(strip=True)\n",
    "            \n",
    "            # Extract analysis\n",
    "            analysis_section = verse_section.find_next_sibling('span')\n",
    "            while analysis_section and analysis_section.name == 'span' and 'الإعراب' not in analysis_section.get_text():\n",
    "                analysis_section = analysis_section.find_next_sibling('span')\n",
    "            \n",
    "            if analysis_section:\n",
    "                analysis_text = analysis_section.get_text(strip=True).replace('\\n', ' ').replace('  ', ' ')\n",
    "            else:\n",
    "                analysis_text = 'No analysis provided'\n",
    "            \n",
    "            verses.append(verse_text)\n",
    "            analysis.append(analysis_text)\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Verse': verses,\n",
    "            'Analysis': analysis\n",
    "        })\n",
    "\n",
    "        print(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function with the appropriate page ID\n",
    "extract_page_content(\"pg-19\")  # Replace with the actual page ID\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"page flex flex-col justify-between svelte-70tuea\" id=\"pg-19\"><div><span class=\"indent\"></span><span data-type=\"title\" id=\"toc-26\">(١) سورة الفاتحة</span><br/><span class=\"indent\"></span>مكية وآياتها (٧)<br/><span class=\"indent\"></span><br/><span class=\"indent\"></span>١ - <span id=\"aya-1\"></span><span data-type=\"title\">﴿بِسْمِ اللهِ الرَّحْمنِ الرَّحِيمِ (١)﴾ </span><br/><span class=\"indent\"></span>الإعراب<br/><span class=\"indent\"></span>﴿بِسْمِ﴾ جار ومجرور متعلق بفعل محذوف تقديره: أبدأ أو بدأت، وحذفت الألف لكثرة الاستعمال. [أو أن شبه الجملة متعلق بمحذوف خبر، والمبتدأ محذوف تقديره: ابتدائي]. ﴿اللهِ﴾: لفظ الجلالة مضاف إليه مجرور. ﴿الرَّحْمنِ﴾: نعت مجرور للفظ الجلالة. ﴿الرَّحِيمِ﴾: نعت ثان مجرور. والجملة الفعلية: (أبدأ) بسم الله ... ابتدائية.<br/><span class=\"indent\"></span><br/><span class=\"indent\"></span>٢ - <span id=\"aya-2\"></span><span data-type=\"title\">﴿الْحَمْدُ لِلّهِ رَبِّ الْعالَمِينَ (٢)﴾ </span><br/><span class=\"indent\"></span>الإعراب<br/><span class=\"indent\"></span>﴿الْحَمْدُ﴾ مبتدأ مرفوع. ﴿لِلّهِ﴾: جار ومجرور متعلق بمحذوف خبر المبتدأ، تقديره: ثابت، أو واجب. والجملة الاسمية: (الحمد ...) استئنافية.<br/><span class=\"indent\"></span>﴿رَبِّ﴾: نعت للفظ الجلالة مجرور. ﴿الْعالَمِينَ﴾: مضاف إليه مجرور بالياء.<br/><span class=\"indent\"></span><br/><span class=\"indent\"></span>٣ - <span id=\"aya-3\"></span><span data-type=\"title\">﴿الرَّحْمنِ الرَّحِيمِ (٣)﴾ </span><br/><span class=\"indent\"></span>الإعراب<br/><span class=\"indent\"></span>﴿الرَّحْمنِ﴾ نعت ثان للفظ الجلالة مجرور.<br/><span class=\"indent\"></span>﴿الرَّحِيمِ﴾: نعت ثالث مجرور.<br/><span class=\"indent\"></span><br/><span class=\"indent\"></span>٤ - <span id=\"aya-4\"></span><span data-type=\"title\">﴿مالِكِ يَوْمِ الدِّينِ (٤)﴾ </span><br/><span class=\"indent\"></span>الإعراب<br/><span class=\"indent\"></span>﴿مالِكِ﴾ نعت رابع مجرور. ﴿يَوْمِ﴾: مضاف إليه مجرور.<br/><span class=\"indent\"></span>﴿الدِّينِ﴾: مضاف إليه مجرور.<br/><span class=\"indent\"></span><br/><span class=\"indent\"></span>٥ - <span id=\"aya-5\"></span><span data-type=\"title\">﴿إِيّاكَ نَعْبُدُ وَإِيّاكَ نَسْتَعِينُ (٥)﴾ </span><br/><span class=\"indent\"></span>الإعراب<br/><span class=\"indent\"></span>﴿إِيّاكَ﴾ ضمير مبني في محل نصب مفعول به مقدم.<br/><span class=\"indent\"></span>﴿نَعْبُدُ﴾: فعل مضارع مرفوع، والفاعل مستتر تقديره: نحن. وجملة: إياك نعبد ... استئنافية.</div> <div class=\"page-no svelte-70tuea\" title=\"19\">١ ‏/ ٢١</div> </div>\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = \"https://app.turath.io/book/86\"\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "\n",
    "WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"pg-19\")))  # Adjust as needed\n",
    "src = driver.page_source\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "page_div = soup.find('div', {'id': \"pg-19\"})\n",
    "print(page_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasser\\AppData\\Local\\Temp\\ipykernel_33636\\3542625071.py:47: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  irab_element = aya_element.find_next('span', text=\"الإعراب\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to quran_grammar.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd  # Import pandas for DataFrame\n",
    "\n",
    "# Set stdout encoding to handle Arabic text\n",
    "# sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "# Initialize WebDriver (adjust the path to chromedriver if needed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://app.turath.io/book/86\"\n",
    "driver.get(url)\n",
    "\n",
    "# Function to extract Aya and its grammatical analysis from the page\n",
    "def extract_page_content(page_id):\n",
    "    try:\n",
    "        # Wait until the page content is loaded\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, page_id)))\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "        # Find the div with the given page ID\n",
    "        page_div = soup.find('div', {'id': page_id})\n",
    "        if not page_div:\n",
    "            print(f\"Page with ID {page_id} not found.\")\n",
    "            return [], []\n",
    "\n",
    "        verses = []\n",
    "        analysis = []\n",
    "\n",
    "        # Find all sections for Ayas\n",
    "        aya_elements = page_div.find_all('span', {'id': lambda x: x and x.startswith('aya-')})\n",
    "\n",
    "        for aya_element in aya_elements:\n",
    "            # Extract the Aya text\n",
    "            aya_text = aya_element.find_next('span', {'data-type': 'title'}).get_text().strip()\n",
    "\n",
    "            # Extract the corresponding I'rab (analysis)\n",
    "            irab_element = aya_element.find_next('span', text=\"الإعراب\")\n",
    "            if irab_element:\n",
    "                irab_text = irab_element.find_next('span').get_text(separator=' ').strip()\n",
    "            else:\n",
    "                irab_text = \"Analysis not found\"  # Handle missing analysis\n",
    "\n",
    "            # Append the Aya and its analysis to the lists\n",
    "            verses.append(aya_text)\n",
    "            analysis.append(irab_text)\n",
    "\n",
    "        return verses, analysis\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting page content: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Function to save the extracted data to a CSV file\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data, columns=['Aya', 'I\\'rab'])\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Example usage to extract content from a specific page\n",
    "page_id = 'pg-19'  # Example page ID (adjust as needed)\n",
    "ayas, irab = extract_page_content(page_id)\n",
    "\n",
    "# If Ayas and their analysis are found, save them to a CSV file\n",
    "if ayas and irab:\n",
    "    data = list(zip(ayas, irab))\n",
    "    save_to_csv(data, 'quran_grammar.csv')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasser\\AppData\\Local\\Temp\\ipykernel_33636\\3959057542.py:47: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  irab_section = aya_element.find_next('span', text=\"الإعراب\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis not found', 'Analysis not found', 'Analysis not found', 'Analysis not found', 'Analysis not found']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd  # Import pandas for DataFrame\n",
    "\n",
    "# Set stdout encoding to handle Arabic text\n",
    "# sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "# Initialize WebDriver (adjust the path to chromedriver if needed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://app.turath.io/book/86\"\n",
    "driver.get(url)\n",
    "\n",
    "# Function to extract Aya and its grammatical analysis from the page\n",
    "def extract_page_content(page_id):\n",
    "    try:\n",
    "        # Wait until the page content is loaded\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, page_id)))\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "        # Find the div with the given page ID\n",
    "        page_div = soup.find('div', {'id': page_id})\n",
    "        if not page_div:\n",
    "            print(f\"Page with ID {page_id} not found.\")\n",
    "            return [], []\n",
    "\n",
    "        verses = []\n",
    "        analysis = []\n",
    "\n",
    "        # Find all sections for Ayas\n",
    "        aya_elements = page_div.find_all('span', {'id': lambda x: x and x.startswith('aya-')})\n",
    "\n",
    "        for aya_element in aya_elements:\n",
    "            # Extract the Aya text\n",
    "            aya_text = aya_element.find_next('span', {'data-type': 'title'}).get_text().strip()\n",
    "\n",
    "            # Find the nearest I'rab (analysis) after the Aya\n",
    "            irab_section = aya_element.find_next('span', text=\"الإعراب\")\n",
    "            if irab_section:\n",
    "                # Collect all text until the next Aya or next section (stop condition)\n",
    "                irab_text = ''\n",
    "                for sibling in irab_section.find_next_siblings():\n",
    "                    # Stop if we encounter the next Aya or any other section\n",
    "                    if sibling.find('span', {'id': lambda x: x and x.startswith('aya-')}):\n",
    "                        break\n",
    "                    irab_text += sibling.get_text(separator=' ').strip() + ' '\n",
    "\n",
    "                irab_text = irab_text.strip()\n",
    "            else:\n",
    "                irab_text = \"Analysis not found\"\n",
    "\n",
    "            # Append the Aya and its analysis to the lists\n",
    "            verses.append(aya_text)\n",
    "            analysis.append(irab_text)\n",
    "\n",
    "        return verses, analysis\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting page content: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Example usage to extract content from a specific page\n",
    "page_id = 'pg-19'  # Example page ID (adjust as needed)\n",
    "ayas, irab = extract_page_content(page_id)\n",
    "\n",
    "print(irab)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasser\\AppData\\Local\\Temp\\ipykernel_33636\\234065016.py:44: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  irab_header = aya_element.find_next('span', text=\"الإعراب\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to quran_grammar.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd  # Import pandas for DataFrame\n",
    "\n",
    "# Set stdout encoding to handle Arabic text\n",
    "# sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "# Initialize WebDriver (adjust the path to chromedriver if needed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://app.turath.io/book/86\"\n",
    "driver.get(url)\n",
    "\n",
    "# Function to extract Aya and its grammatical analysis from the page\n",
    "def extract_page_content(page_id):\n",
    "    try:\n",
    "        # Wait until the page content is loaded\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, page_id)))\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "        # Find the div with the given page ID\n",
    "        page_div = soup.find('div', {'id': page_id})\n",
    "      \n",
    "        verses = []\n",
    "        analysis = []\n",
    "\n",
    "        # Find all sections for Ayas\n",
    "        aya_elements = page_div.find_all('span', {'id': lambda x: x and x.startswith('aya-')})\n",
    "\n",
    "        for aya_element in aya_elements:\n",
    "            # Extract the Aya text\n",
    "            aya_text = aya_element.find_next('span', {'data-type': 'title'}).get_text().strip()\n",
    "\n",
    "            # Find the 'الإعراب' section and its analysis\n",
    "            irab_header = aya_element.find_next('span', text=\"الإعراب\")\n",
    "            if irab_header:\n",
    "                # Collect the grammatical analysis following the 'الإعراب' header\n",
    "                irab_text = ''\n",
    "                for sibling in irab_header.find_next_siblings():\n",
    "                    if sibling.find('span', {'id': lambda x: x and x.startswith('aya-')}):\n",
    "                        break  # Stop when the next Aya starts\n",
    "                    irab_text += sibling.get_text(separator=' ').strip() + ' '\n",
    "\n",
    "                irab_text = irab_text.strip()\n",
    "            else:\n",
    "                irab_text = \"Analysis not found\"\n",
    "\n",
    "            # Append the Aya and its analysis to the lists\n",
    "            verses.append(aya_text)\n",
    "            analysis.append(irab_text)\n",
    "\n",
    "        return verses, analysis\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting page content: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Function to save the extracted data to a CSV file\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data, columns=['Aya', 'I\\'rab'])\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Example usage to extract content from a specific page\n",
    "page_id = 'pg-19'  # Example page ID (adjust as needed)\n",
    "ayas, irab = extract_page_content(page_id)\n",
    "\n",
    "# If Ayas and their analysis are found, save them to a CSV file\n",
    "if ayas and irab:\n",
    "    data = list(zip(ayas, irab))\n",
    "    save_to_csv(data, 'quran_grammar.csv')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd  # Import pandas for DataFrame\n",
    "\n",
    "# Set stdout encoding to handle Arabic text\n",
    "# sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "# Initialize WebDriver (adjust the path to chromedriver if needed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://app.turath.io/book/86\"\n",
    "driver.get(url)\n",
    "\n",
    "# Function to extract Aya and its grammatical analysis from the page\n",
    "def extract_page_content(page_id):\n",
    "    try:\n",
    "        # Wait until the page content is loaded\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, page_id)))\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        src = driver.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "        # Find the div with the given page ID\n",
    "        page_div = soup.find('div', {'id': page_id})\n",
    "       \n",
    "\n",
    "        verses = []\n",
    "        analysis = []\n",
    "\n",
    "        # Find all sections for Ayas by locating the span with ids starting with 'aya-'\n",
    "        aya_elements = page_div.find_all('span', {'id': lambda x: x and x.startswith('aya-')})\n",
    "\n",
    "        for aya_element in aya_elements:\n",
    "            # Extract the Aya text\n",
    "            aya_text = aya_element.find_next('span', {'data-type': 'title'}).get_text().strip()\n",
    "\n",
    "            # Find the corresponding I'rab (analysis), which follows the \"الإعراب\" text\n",
    "            irab_header = aya_element.find_next('span', text=\"الإعراب\")\n",
    "            if irab_header:\n",
    "                # Collect all text for the analysis after the I'rab header\n",
    "                irab_text = irab_header.find_next('span').get_text(separator=' ').strip()\n",
    "            else:\n",
    "                irab_text = \"Analysis not found\"\n",
    "\n",
    "            # Append the Aya and its analysis to the lists\n",
    "            verses.append(aya_text)\n",
    "            analysis.append(irab_text)\n",
    "\n",
    "        return verses, analysis\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while extracting page content: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "\n",
    "# Example usage to extract content from a specific page\n",
    "page_id = 'pg-19'  # Example page ID (adjust as needed)\n",
    "ayas, irab = extract_page_content(page_id)\n",
    "print(irab)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
